
# Projects and Labs in Data Science using Machine Learning Pipelines 

##  ML_Pipelines_using_BERT

### Build, Train, and Deploy ML Pipelines using BERT

### Summary of my Labs and Projects from Coursera

- Automate a natural language processing task by building an end-to-end machine learning pipeline using Hugging Face’s highly-optimized implementation of the state-of-the-art BERT algorithm with Amazon SageMaker Pipelines. The pipeline will first transform the dataset into BERT-readable features and store the features in the Amazon SageMaker Feature Store. It will then fine-tune a text classification model to the dataset using a Hugging Face pre-trained model, which has learned to understand the human language from millions of Wikipedia documents. Finally, your pipeline will evaluate the model’s accuracy and only deploy the model if the accuracy exceeds a given threshold.

- Practical data science is geared towards handling massive datasets that do not fit in your local hardware and could originate from multiple sources. One of the biggest benefits of developing and running data science projects in the cloud is the agility and elasticity that the cloud offers to scale up and out at a minimum cost.

- The focus of these projects are on developing ML workflows using Amazon SageMaker, Python and SQL programming languages. I fully recommend you these projects if you want to learn how to build, train, and deploy scalable, end-to-end ML pipelines in the AWS cloud.

- Steps ML_Pipelines_using_BERT

    1. Configure the SageMaker Feature Store

    2. Transform the dataset

    3. Inspect the transformed data

    4. Query the Feature Store

- Train a review classifier with BERT and Amazon SageMaker

    5. Configure dataset

    6. Configure model hyper-parameters

    7. Setup evaluation metrics, debugger and profiler

    8. Train model

    9. Analyze debugger results

    10. Deploy and test the model
    
References:
    
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)

- [Fundamental Techniques of Feature Engineering for Machine Learning](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)

- [PyTorch Hub](https://pytorch.org/hub/)

- [TensorFlow Hub](https://www.tensorflow.org/hub)

- [Hugging Face open-source NLP transformers library](https://github.com/huggingface/transformers) 

- [RoBERTa model](https://arxiv.org/abs/1907.11692) 

- [Amazon SageMaker Model Training (Developer Guide)](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)

- [Amazon SageMaker Debugger: A system for real-time insights into machine learning model training](https://www.amazon.science/publications/amazon-sagemaker-debugger-a-system-for-real-time-insights-into-machine-learning-model-training)

- [The science behind SageMaker’s cost-saving Debugger](https://www.amazoN.science/blog/the-science-behind-sagemakers-cost-saving-debugger)

- [Amazon SageMaker Debugger (Developer Guide)](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html)

- [Amazon SageMaker Debugger (GitHub)](https://github.com/awslabs/sagemaker-debugger)






